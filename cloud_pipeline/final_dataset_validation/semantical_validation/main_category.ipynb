{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab4e91-90c1-44df-9816-cba875d6d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a lot of unncesessary info presented, \n",
    "# main point is to show that pattern works across main categories also, \n",
    "# not only for high category level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a400dcf-9d1f-4311-bba4-10cf04bf2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef1c16b-1c81-4b01-9312-c2f41e9ecdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\n",
    "    \"/Users/macos/Documents/sentiment_project/validation/main_category.csv\"\n",
    ")\n",
    "df.groupby(\"main_category\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de73793b-610f-4d2e-8303-8fb358ba65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_sizes(n_total):\n",
    "    if n_total <= 100:\n",
    "        return [min(20, n_total // 2), n_total]\n",
    "    elif n_total <= 300:\n",
    "        return [50, 75, 100, 125, 150, 200, min(250, n_total)]\n",
    "    elif n_total <= 800:\n",
    "        return [50, 100, 150, 200, 250, 300, 400, 500, min(700, n_total)]\n",
    "    elif n_total <= 1500:\n",
    "        return [100, 250, 500, 600, 650, 750, min(1000, n_total)]\n",
    "    elif n_total <= 4000:\n",
    "        return [100, 250, 500, 750, 1000, 1500, 2000, min(3000, n_total)]\n",
    "    else:\n",
    "        return [100, 250, 500, 750, 1000, 1500, 2000, 2500, 3000, 3500, 4000, n_total]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aad847-7474-4238-a31c-6d60e80f5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Simple tokenizer converting text to lowercase words.\"\"\"\n",
    "    return re.findall(r\"\\b\\w+\\b\", str(text).lower())\n",
    "\n",
    "\n",
    "# === Constants ===\n",
    "TOP_N = 100\n",
    "TARGET_COLUMN = \"aspect_summary\"\n",
    "\n",
    "print(\n",
    "    \"\\nðŸ§  WORD STABILITY ANALYSIS BY HIGH_CATEGORY â†’ MAIN_CATEGORY (Progressive Sample Sizes)\\n\"\n",
    ")\n",
    "all_results = []\n",
    "\n",
    "# === Iterate through each high_category ===\n",
    "for high_cat, df_high in df.groupby(\"high_category\"):\n",
    "    print(f\"\\nðŸ”¹ HIGH CATEGORY: {high_cat}\")\n",
    "\n",
    "    # === Iterate through each main_category within this high_category ===\n",
    "    for main_cat, df_main in df_high.groupby(\"main_category\"):\n",
    "        print(f\"   â†³ MAIN CATEGORY: {main_cat}\")\n",
    "        n_total = len(df_main)\n",
    "        SAMPLE_SIZES = get_sample_sizes(n_total)\n",
    "\n",
    "        results = []\n",
    "        prev_words = None\n",
    "        prev_freqs = None\n",
    "\n",
    "        for n in SAMPLE_SIZES:\n",
    "            if len(df_main) < n:\n",
    "                continue  # skip if not enough data\n",
    "\n",
    "            sample = df_main.iloc[:n]\n",
    "\n",
    "            # --- Tokenize and count words ---\n",
    "            words = [w for text in sample[TARGET_COLUMN] for w in tokenize(text)]\n",
    "            freqs = Counter(words)\n",
    "            top_words = dict(freqs.most_common(TOP_N))\n",
    "\n",
    "            # --- Compare with previous sample ---\n",
    "            if prev_words is not None:\n",
    "                # 1ï¸âƒ£ Overlap (Jaccard similarity)\n",
    "                overlap = len(set(prev_words) & set(top_words)) / len(\n",
    "                    set(prev_words) | set(top_words)\n",
    "                )\n",
    "\n",
    "                # 2ï¸âƒ£ Average frequency change\n",
    "                common_words = set(prev_words) & set(top_words)\n",
    "                changes = []\n",
    "                for w in common_words:\n",
    "                    prev_count = prev_freqs[w]\n",
    "                    curr_count = top_words[w]\n",
    "                    rel_change = abs(curr_count - prev_count) / max(prev_count, 1)\n",
    "                    changes.append(rel_change)\n",
    "                avg_change = np.mean(changes) if changes else 0\n",
    "\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"sample_size\": n,\n",
    "                        \"word_overlap_%\": round(overlap * 100, 2),\n",
    "                        \"avg_freq_change_%\": round(avg_change * 100, 2),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            prev_words = top_words\n",
    "            prev_freqs = freqs\n",
    "\n",
    "        # === Results per main_category ===\n",
    "        if results:\n",
    "            results_df = pd.DataFrame(results)\n",
    "            print(results_df.to_string(index=False))\n",
    "\n",
    "            # store all results together\n",
    "            for row in results:\n",
    "                all_results.append(\n",
    "                    {\"high_category\": high_cat, \"main_category\": main_cat, **row}\n",
    "                )\n",
    "\n",
    "# === Global summary ===\n",
    "print(\"\\nðŸ“˜ INTERPRETATION:\")\n",
    "print(\n",
    "    \"â€¢ Each table shows how stable top vocabulary becomes as sample size grows within each main_category.\"\n",
    ")\n",
    "print(\n",
    "    \"â€¢ 'word_overlap_%' â†’ similarity of top words between consecutive samples (higher = more stable).\"\n",
    ")\n",
    "print(\n",
    "    \"â€¢ 'avg_freq_change_%' â†’ how much word frequencies fluctuate across sizes (lower = more stable).\"\n",
    ")\n",
    "print(\n",
    "    \"â€¢ Expect overlap to increase and freq change to flatten as the data stabilizes.\\n\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
