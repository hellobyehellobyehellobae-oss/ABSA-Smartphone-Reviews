{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d145adf-b6f4-491b-a9f0-4851da324f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"ABSA-Smartphone-Reviews/data/high_category.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a80b08-8609-469b-9c44-09a7a354637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", str(text).lower())\n",
    "\n",
    "\n",
    "SAMPLE_SIZES = [100, 200, 300, 500, 1000, 2000, 4000, 5000, 6000, 10000, 15000, 20000]\n",
    "TOP_N = 100\n",
    "TARGET_COLUMN = \"aspect_summary\"\n",
    "\n",
    "print(\"\\nðŸ§  MODEL OUTPUT STABILITY ANALYSIS BY PROGRESSIVE SAMPLE SIZE\\n\")\n",
    "print(f\"Analyzing top {TOP_N} words per growing dataset slice...\\n\")\n",
    "\n",
    "results = []\n",
    "prev_words = None\n",
    "prev_freqs = None\n",
    "\n",
    "for n in SAMPLE_SIZES:\n",
    "    sample = df.iloc[:n]  # ðŸ‘ˆ take first N rows instead of random sample\n",
    "\n",
    "    # tokenize and count words\n",
    "    words = [w for text in sample[TARGET_COLUMN] for w in tokenize(text)]\n",
    "    freqs = Counter(words)\n",
    "    top_words = dict(freqs.most_common(TOP_N))\n",
    "\n",
    "    # compute metrics if previous sample exists\n",
    "    if prev_words is not None:\n",
    "        # --- 1) Overlap (Jaccard similarity) ---\n",
    "        overlap = len(set(prev_words) & set(top_words)) / len(\n",
    "            set(prev_words) | set(top_words)\n",
    "        )\n",
    "\n",
    "        # --- 2) Quantified frequency change ---\n",
    "        common_words = set(prev_words) & set(top_words)\n",
    "        changes = []\n",
    "        for w in common_words:\n",
    "            prev_count = prev_freqs[w]\n",
    "            curr_count = top_words[w]\n",
    "            rel_change = abs(curr_count - prev_count) / max(prev_count, 1)\n",
    "            changes.append(rel_change)\n",
    "        avg_change = np.mean(changes) if changes else 0\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"sample_size\": n,\n",
    "                \"word_overlap_%\": round(overlap * 100, 2),\n",
    "                \"avg_freq_change_%\": round(avg_change * 100, 2),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    prev_words = top_words\n",
    "    prev_freqs = freqs\n",
    "\n",
    "# === RESULTS OUTPUT ===\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“˜ INTERPRETATION:\")\n",
    "print(\"â€¢ Rows are taken sequentially (first N), not randomly.\")\n",
    "print(\"â€¢ 'word_overlap_%' shows how stable top vocabulary becomes as data grows.\")\n",
    "print(\"â€¢ 'avg_freq_change_%' measures variation in word frequencies across sizes.\")\n",
    "print(\n",
    "    \"â€¢ Expect overlap to increase and frequency changes to flatten as stability improves.\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b21175-8bfe-476f-82cd-b7222233b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"high_category\").size()\n",
    "\n",
    "SAMPLE_SIZES_CATEGORY = [\n",
    "    100,\n",
    "    250,\n",
    "    500,\n",
    "    1000,\n",
    "    2000,\n",
    "    3000,\n",
    "    4000,\n",
    "    5000,\n",
    "    6000,\n",
    "    7000,\n",
    "    8000,\n",
    "    9000,\n",
    "    10000,\n",
    "    12000,\n",
    "    13000,\n",
    "]\n",
    "TOP_N = 100\n",
    "TARGET_COLUMN = \"aspect_summary\"\n",
    "\n",
    "print(\"\\nðŸ§  WORD STABILITY ANALYSIS BY HIGH_CATEGORY (Progressive Sample Sizes)\\n\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Loop through each high_category\n",
    "for high_cat, df_high in df.groupby(\"high_category\"):\n",
    "    print(f\"\\nðŸ”¹ Analyzing High Category: {high_cat}\")\n",
    "    results = []\n",
    "    prev_words = None\n",
    "    prev_freqs = None\n",
    "\n",
    "    for n in SAMPLE_SIZES_CATEGORY:\n",
    "        # Only proceed if we have at least n rows in this category\n",
    "        if len(df_high) < n:\n",
    "            continue\n",
    "\n",
    "        sample = df_high.iloc[:n]  # first N rows for this category\n",
    "\n",
    "        # Tokenize and count words\n",
    "        words = [w for text in sample[TARGET_COLUMN] for w in tokenize(text)]\n",
    "        freqs = Counter(words)\n",
    "        top_words = dict(freqs.most_common(TOP_N))\n",
    "\n",
    "        # Compare with previous sample (if available)\n",
    "        if prev_words is not None:\n",
    "            # --- 1) Overlap (Jaccard similarity) ---\n",
    "            overlap = len(set(prev_words) & set(top_words)) / len(\n",
    "                set(prev_words) | set(top_words)\n",
    "            )\n",
    "\n",
    "            # --- 2) Frequency change ---\n",
    "            common_words = set(prev_words) & set(top_words)\n",
    "            changes = [\n",
    "                abs(top_words[w] - prev_freqs[w]) / max(prev_freqs[w], 1)\n",
    "                for w in common_words\n",
    "            ]\n",
    "            avg_change = np.mean(changes) if changes else 0\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"high_category\": high_cat,\n",
    "                    \"sample_size\": n,\n",
    "                    \"word_overlap_%\": round(overlap * 100, 2),\n",
    "                    \"avg_freq_change_%\": round(avg_change * 100, 2),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        prev_words = top_words\n",
    "        prev_freqs = freqs\n",
    "\n",
    "    # Save and print per-category table\n",
    "    if results:\n",
    "        df_results = pd.DataFrame(results)\n",
    "        all_results.append(df_results)\n",
    "        print(df_results.to_string(index=False))\n",
    "    else:\n",
    "        print(\"   âš ï¸ Not enough samples for progressive analysis in this category.\")\n",
    "\n",
    "# Combine all into one final dataframe\n",
    "final_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "print(\"\\nðŸ“Š SUMMARY: Average Stability by High Category\\n\")\n",
    "summary = (\n",
    "    final_df.groupby(\"high_category\")[[\"word_overlap_%\", \"avg_freq_change_%\"]]\n",
    "    .mean()\n",
    "    .round(2)\n",
    ")\n",
    "print(summary.to_string())\n",
    "\n",
    "print(\"\\nðŸ“˜ INTERPRETATION:\")\n",
    "print(\"â€¢ Each table shows how top-word stability evolves for one category.\")\n",
    "print(\n",
    "    \"â€¢ The summary table averages overlap and frequency change across all sample sizes.\"\n",
    ")\n",
    "print(\n",
    "    \"â€¢ Categories with higher overlap% and lower freq_change% are more lexically stable.\"\n",
    ")\n",
    "print(\"â€¢ You can visually compare which high_categories stabilize faster.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939066c1-5670-4c29-95e0-bb6071428f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for high_cat, sub_df in final_df.groupby(\"high_category\"):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "    fig.suptitle(f\"WORD STABILITY â€” {high_cat}\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # === Compute dynamic y-limits ===\n",
    "    overlap_min = sub_df[\"word_overlap_%\"].min()\n",
    "    overlap_max = sub_df[\"word_overlap_%\"].max()\n",
    "    freq_min = sub_df[\"avg_freq_change_%\"].min()\n",
    "    freq_max = sub_df[\"avg_freq_change_%\"].max()\n",
    "\n",
    "    # Add a little padding (optional)\n",
    "    overlap_pad = (overlap_max - overlap_min) * 0.1\n",
    "    freq_pad = (freq_max - freq_min) * 0.1\n",
    "\n",
    "    # --- Left plot: Word Overlap ---\n",
    "    axes[0].plot(\n",
    "        sub_df[\"sample_size\"], sub_df[\"word_overlap_%\"], marker=\"o\", color=\"tab:blue\"\n",
    "    )\n",
    "    axes[0].set_title(\"Word Overlap (%)\")\n",
    "    axes[0].set_xlabel(\"Sample Size\")\n",
    "    axes[0].set_ylabel(\"Overlap (%)\")\n",
    "    axes[0].grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    axes[0].set_ylim(overlap_min - overlap_pad, overlap_max + overlap_pad)\n",
    "\n",
    "    # --- Right plot: Avg Freq Change ---\n",
    "    axes[1].plot(\n",
    "        sub_df[\"sample_size\"],\n",
    "        sub_df[\"avg_freq_change_%\"],\n",
    "        marker=\"s\",\n",
    "        color=\"tab:orange\",\n",
    "    )\n",
    "    axes[1].set_title(\"Avg Frequency Change (%)\")\n",
    "    axes[1].set_xlabel(\"Sample Size\")\n",
    "    axes[1].set_ylabel(\"Change (%)\")\n",
    "    axes[1].grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    axes[1].set_ylim(freq_min - freq_pad, freq_max + freq_pad)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f532759-adfb-478b-aa7a-4030a22eb929",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZES_CATEGORY = [\n",
    "    100,\n",
    "    250,\n",
    "    500,\n",
    "    1000,\n",
    "    2000,\n",
    "    3000,\n",
    "    4000,\n",
    "    5000,\n",
    "    6000,\n",
    "    7000,\n",
    "    8000,\n",
    "    9000,\n",
    "    10000,\n",
    "    12000,\n",
    "    13000,\n",
    "]\n",
    "\n",
    "# --- Compute percentage distributions per sample size ---\n",
    "results = []\n",
    "\n",
    "for n in SAMPLE_SIZES_CATEGORY:\n",
    "    if len(df) < n:\n",
    "        continue\n",
    "\n",
    "    sample = df.iloc[:n]\n",
    "    dist = sample[\"high_category\"].value_counts(normalize=True) * 100\n",
    "    temp = pd.DataFrame(\n",
    "        {\"sample_size\": n, \"category\": dist.index, \"percentage\": dist.values}\n",
    "    )\n",
    "    results.append(temp)\n",
    "\n",
    "df_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# --- Plot each category independently with smoothing ---\n",
    "for cat, cat_df in df.groupby(\"high_category\"):\n",
    "    total_rows = len(cat_df)\n",
    "\n",
    "    # Get only sample sizes up to category's data size\n",
    "    valid_sizes = [n for n in SAMPLE_SIZES_CATEGORY if n <= total_rows]\n",
    "    cat_results = df_results[\n",
    "        (df_results[\"category\"] == cat) & (df_results[\"sample_size\"].isin(valid_sizes))\n",
    "    ]\n",
    "\n",
    "    if cat_results.empty:\n",
    "        continue\n",
    "\n",
    "    cat_results = cat_results.sort_values(\"sample_size\")\n",
    "\n",
    "    # Smooth trend using rolling mean (window=3, adjustable)\n",
    "    smooth_y = (\n",
    "        cat_results[\"percentage\"].rolling(window=3, min_periods=1, center=True).mean()\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(\n",
    "        cat_results[\"sample_size\"],\n",
    "        cat_results[\"percentage\"],\n",
    "        marker=\"o\",\n",
    "        label=\"Raw %\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    plt.plot(\n",
    "        cat_results[\"sample_size\"],\n",
    "        smooth_y,\n",
    "        color=\"red\",\n",
    "        linewidth=1,\n",
    "        label=\"Smoothed trend\",\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Category: {cat} ({total_rows} rows)\")\n",
    "    plt.xlabel(\"Sample Size\")\n",
    "    plt.ylabel(\"Percentage (%)\")\n",
    "\n",
    "    # Independent axes\n",
    "    plt.xlim(0, valid_sizes[-1] + valid_sizes[-1] * 0.05)\n",
    "    ymin, ymax = cat_results[\"percentage\"].min(), cat_results[\"percentage\"].max()\n",
    "    plt.ylim(max(0, ymin - 1), min(100, ymax + 1))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3c7ce-b9a2-4630-94ad-11fcc5fd6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for cat, cat_df in df.groupby(\"high_category\"):\n",
    "    total_rows = len(cat_df)\n",
    "\n",
    "    # include only sample sizes within category size\n",
    "    valid_sizes = [n for n in SAMPLE_SIZES_CATEGORY if n <= total_rows]\n",
    "    cat_results = df_results[\n",
    "        (df_results[\"category\"] == cat) & (df_results[\"sample_size\"].isin(valid_sizes))\n",
    "    ]\n",
    "\n",
    "    if cat_results.empty:\n",
    "        continue\n",
    "\n",
    "    cat_results = cat_results.sort_values(\"sample_size\")\n",
    "\n",
    "    # smoothing\n",
    "    smooth_y = (\n",
    "        cat_results[\"percentage\"].rolling(window=3, min_periods=1, center=True).mean()\n",
    "    )\n",
    "\n",
    "    # plot line that naturally ends when data ends\n",
    "    plt.plot(cat_results[\"sample_size\"], smooth_y, marker=\"o\", linewidth=2, label=cat)\n",
    "\n",
    "\n",
    "plt.title(\"Category Distribution Changes Across Sample Sizes\")\n",
    "plt.xlabel(\"Sample Size\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title=\"Category\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
